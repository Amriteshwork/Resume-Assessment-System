# Guidelines for Machine Learning Roles

Use these with the general guidelines when the JD is for **ML / AI / Data Science** roles.

## 1. What to Focus On

- **Core ML skills**
  - Supervised/unsupervised learning, model evaluation, feature engineering.
  - Classical models (regression, trees, ensembles, clustering, etc.).

- **Deep learning (if JD mentions it)**
  - Experience with CNNs/RNNs/Transformers or similar.
  - Frameworks: PyTorch, TensorFlow, Keras, etc.

- **Data skills**
  - Python + data stack (NumPy, pandas, SQL).
  - Evidence of cleaning, transforming, and exploring data.

- **MLOps / production (if required)**
  - Serving models (APIs or batch), monitoring, retraining.
  - Experiment tracking, CI/CD for ML, cloud deployment.

- **Project relevance**
  - Product ML (ranking, recommendation, search, personalization).
  - NLP, CV, time series, forecasting, etc.
  - Call out whether their past work matches the JD domain.

## 2. Seniority Signals (Heuristic)

Use these patterns to justify `seniority_score` and comments:

- **Junior**
  - 0–2 years OR mostly academic/bootcamp projects.
  - Limited ownership; few or no production deployments.

- **Mid-level**
  - ~2–5 years relevant ML work.
  - Has shipped models or directly supported production use cases.
  - Shows end-to-end ownership (data → model → metrics → deployment).

- **Senior**
  - 5+ years in ML / applied science roles.
  - Drives model and experiment design; mentors others.
  - Shows clear impact on metrics (accuracy, F1, CTR, churn, revenue, etc.).

## 3. Suggestions for Improvement (ML-Specific)

Typical improvement ideas:

- Clarify **model types, frameworks, and metrics** for key projects.
- Highlight **production deployments** and how models are monitored or retrained.
- Add **impact numbers** (before/after metrics or business results).
- Group skills into clear categories: ML, data, software/engineering, cloud/MLOps.
- Link to **public work** (GitHub, Kaggle, papers) if appropriate.

Keep feedback concrete, e.g.:

- “Clarify which models and metrics were used for the fraud detection system.”
- “Highlight any production deployment details (scale, latency, monitoring).”
- “Quantify improvements from your models where possible.”
